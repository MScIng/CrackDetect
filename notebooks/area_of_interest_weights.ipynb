{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After mapping each AOI to 2 time windows for each car, we will predict the KPI of the AOI\n",
    "\n",
    "There are in total 1226 AOIs for HH and 940 for VH.\n",
    "\n",
    "We have X windows for each AOI based on the filter. \n",
    "\n",
    "The weights should be found based on the Two closest points, and the avg. speed that it has at these two points.\n",
    "\n",
    "The logic is that there are always(*) a second on each side of the AOI. By using the distance and the speed, one can determine \n",
    "\n",
    "1 and 1,1\n",
    "\n",
    "Total distance from \n",
    "\n",
    "or 1 and 2\n",
    "\n",
    "(*) If the speed differs alot between the seconds they could both be on the same side, but that seems to be very unlikely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import cm\n",
    "import h5py\n",
    "from scipy.interpolate import CubicSpline\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def load_from_hdf5(filename):\n",
    "\n",
    "    def unpack_group(group):\n",
    "        unpacked_data = {}\n",
    "        for key, item in group.items():\n",
    "            if isinstance(item, h5py.Group):\n",
    "                unpacked_data[key] = unpack_group(item)\n",
    "            else:\n",
    "                unpacked_data[key] = item[:].tolist()  # Convert dataset to list\n",
    "        return unpacked_data\n",
    "\n",
    "    with h5py.File(filename, 'r') as hdf_file:\n",
    "        loaded_mapping = unpack_group(hdf_file)\n",
    "    \n",
    "    return loaded_mapping\n",
    "\n",
    "# Takes about 40 seconds to load\n",
    "h5py_mapping_hh = load_from_hdf5(\"../data/processed/AOI/mapping_hh_time_to_location.hdf5\")\n",
    "h5py_mapping_vh = load_from_hdf5(\"../data/processed/AOI/mapping_hh_time_to_location.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_distance(data):\n",
    "    first_values = []\n",
    "\n",
    "    for trip_name, passes in data.items():\n",
    "        for pass_name, segments in passes.items():\n",
    "            for segment_name, values in segments.items():\n",
    "                if segment_name in ['distance_segment_second_1', 'distance_segment_second_2']:\n",
    "                    # Extract the first value and add to the list\n",
    "                    first_values.append(values[0])\n",
    "\n",
    "    total_distance = np.sum(first_values)\n",
    "    return total_distance\n",
    "\n",
    "\n",
    "def time_to_drive_X_meters(speed_kmh, distance_m=10):\n",
    "    # Convert speed from km/h to m/s\n",
    "    speed_ms = speed_kmh * (1000 / 3600)\n",
    "\n",
    "    # Calculate time in seconds\n",
    "    time_seconds = distance_m / speed_ms\n",
    "    \n",
    "    return time_seconds\n",
    "\n",
    "\n",
    "# Weight functions \n",
    "def ln_of_ratio(values, total):\n",
    "    weight_values = -np.log(values / total)\n",
    "    norm = np.linalg.norm(weight_values)\n",
    "    unit_vector = weight_values / norm\n",
    "    return unit_vector\n",
    "\n",
    "\n",
    "def ln_of_ratio_sum_to_1(values, total):\n",
    "    weight_values = -np.log(values / total)\n",
    "    total = np.sum(weight_values)\n",
    "    weight_values_1 = weight_values / total\n",
    "    return weight_values_1\n",
    "\n",
    "\n",
    "def inverse_of_ratio(values, total):\n",
    "    weight_values = (values / total)**(-1)\n",
    "    norm = np.linalg.norm(weight_values)\n",
    "    unit_vector = weight_values / norm\n",
    "    return unit_vector\n",
    "\n",
    "\n",
    "def inverse_of_ratio_sum_to_1(values, total):\n",
    "    weight_values = (values / total)**(-1)\n",
    "    total = np.sum(weight_values)\n",
    "    weight_values_1 = weight_values / total\n",
    "    return weight_values_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80961586 0.49641369 0.31320217]\n",
      "[0.5       0.3065736 0.1934264]\n",
      "[0.85714286 0.42857143 0.28571429]\n",
      "[0.54545455 0.27272727 0.18181818]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([1, 2, 3])\n",
    "total = 6.\n",
    "\n",
    "print(ln_of_ratio(test, total))\n",
    "print(ln_of_ratio_sum_to_1(test, total))\n",
    "\n",
    "print(inverse_of_ratio(test, total))\n",
    "print(inverse_of_ratio_sum_to_1(test, total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass_1': {'distance_segment_second_1': [2.3317746094376914e-05, 0.0, 0.0], 'distance_segment_second_2': [3.658563558098666e-05, 0.0, 1.0]}, 'pass_11': {'distance_segment_second_1': [6.483034333046726e-05, 2.0, 2.0], 'distance_segment_second_2': [9.578165683427985e-05, 2.0, 1.0]}, 'pass_13': {'distance_segment_second_1': [3.2137861824301134e-05, 3.0, 1.0], 'distance_segment_second_2': [4.7965809663709314e-05, 3.0, 0.0]}, 'pass_15': {'distance_segment_second_1': [7.158024588141076e-05, 5.0, 0.0], 'distance_segment_second_2': [8.890653162778374e-05, 5.0, 1.0]}, 'pass_17': {'distance_segment_second_1': [2.3883416921457747e-05, 7.0, 1.0], 'distance_segment_second_2': [6.0817756440021506e-05, 7.0, 0.0]}, 'pass_3': {'distance_segment_second_1': [4.2139430466692684e-05, 10.0, 0.0], 'distance_segment_second_2': [6.319870650188742e-05, 10.0, 1.0]}, 'pass_5': {'distance_segment_second_1': [9.47361156045296e-05, 12.0, 0.0], 'distance_segment_second_2': [9.47361156045296e-05, 12.0, 1.0]}, 'pass_9': {'distance_segment_second_1': [4.313462564602431e-05, 16.0, 1.0], 'distance_segment_second_2': [6.148663076436247e-05, 16.0, 0.0]}}\n",
      "{'distance_segment_second_1': [2.3317746094376914e-05, 0.0, 0.0], 'distance_segment_second_2': [3.658563558098666e-05, 0.0, 1.0]}\n",
      "[2.3317746094376914e-05, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for car in h5py_mapping_hh[\"0\"]:\n",
    "    print(h5py_mapping_hh[\"0\"][car])\n",
    "    for trip in h5py_mapping_hh[\"0\"][car]:\n",
    "        print(h5py_mapping_hh[\"0\"][car][trip]) \n",
    "        for distance_segment_second in h5py_mapping_hh[\"0\"][car][trip]:\n",
    "            print(h5py_mapping_hh[\"0\"][car][trip][distance_segment_second])\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_with_weights_hh = h5py_mapping_hh\n",
    "mapping_with_weights_vh = h5py_mapping_vh\n",
    "maps = [mapping_with_weights_hh, mapping_with_weights_vh]\n",
    "\n",
    "new_mapping = {}\n",
    "\n",
    "for mapping in maps:\n",
    "    indexes_strings = list(mapping.keys())\n",
    "    indexes = np.sort([int(x) for x in mapping])\n",
    "    for index in indexes:\n",
    "        new_mapping[index] = []\n",
    "        distances = []\n",
    "        segments = []\n",
    "        seconds = []\n",
    "\n",
    "        total_distance_for_index = get_total_distance(mapping[str(index)])\n",
    "        # Now go into each individual trip and segment and calculate the weight\n",
    "        for car in mapping[str(index)]:\n",
    "            for trip in mapping[str(index)][car]:\n",
    "                for distance_segment_second in mapping[str(index)][car][trip]:\n",
    "                    distances.append(mapping[str(index)][car][trip][distance_segment_second][0])\n",
    "                    segments.append(mapping[str(index)][car][trip][distance_segment_second][1])\n",
    "                    seconds.append(mapping[str(index)][car][trip][distance_segment_second][2])\n",
    "\n",
    "        \n",
    "        weight_ln_ratio = ln_of_ratio_sum_to_1(distances, total_distance_for_index)\n",
    "        weight_inverse_ratio = inverse_of_ratio_sum_to_1(distances, total_distance_for_index)\n",
    "\n",
    "        for i in range(len(distances)):\n",
    "            new_mapping[index].append([distances[i], segments[i], seconds[i], weight_ln_ratio[i], weight_inverse_ratio[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.508740212138433e-05, 4.0, 49.0, 0.06547794769130515, 0.06307755338794452],\n",
       " [9.232527915058013e-05, 4.0, 48.0, 0.06347318278045366, 0.05813256346833801],\n",
       " [6.600137795377312e-05, 8.0, 88.0, 0.07171537179454744, 0.08131807723336026],\n",
       " [6.972502513463056e-05, 8.0, 87.0, 0.07036762292939944, 0.07697530606249184],\n",
       " [7.020840886891911e-05, 13.0, 59.0, 0.07019796738324847, 0.07644533235290442],\n",
       " [7.021387426553937e-05, 13.0, 60.0, 0.07019605585179835, 0.07643938190414415],\n",
       " [9.422030566549493e-05, 20.0, 40.0, 0.06297425113592345, 0.05696335956505656],\n",
       " [9.538483107683817e-05,\n",
       "  22.0,\n",
       "  43.0,\n",
       "  0.06267260336149204,\n",
       "  0.056267910624380094],\n",
       " [8.033683081554249e-05, 25.0, 86.0, 0.06688873626929294, 0.06680752894368305],\n",
       " [8.780281214380418e-05, 25.0, 85.0, 0.06470651771302419, 0.0611268024213488],\n",
       " [8.721455087953374e-05, 27.0, 85.0, 0.06487159470441167, 0.06153910208591807],\n",
       " [8.891766387740006e-05, 27.0, 84.0, 0.06439668177105286, 0.06036039315375293],\n",
       " [8.506221598201344e-05, 61.0, 45.0, 0.06548521755754794, 0.06309623007103421],\n",
       " [7.587189298273057e-05,\n",
       "  76.0,\n",
       "  119.0,\n",
       "  0.06829292098023508,\n",
       "  0.07073904365579418],\n",
       " [7.59015378867963e-05, 76.0, 120.0, 0.06828332807626729, 0.07071141506984899]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mapping[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
