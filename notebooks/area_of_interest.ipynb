{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From time predictions to location predictions\n",
    "\n",
    "We will try predicting the road condition in 1 meter intervals.\n",
    "\n",
    "The issue before have been we have only used intervals, which just had the length of whatever far one could travel in 1 second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import cm\n",
    "import h5py\n",
    "from scipy.interpolate import CubicSpline\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {\n",
    "        'acc_long':     {'bstar': 198,      'rstar': 1,     'b': 198,   'r': 0.05   },\n",
    "        'acc_trans':    {'bstar': 32768,    'rstar': 1,     'b': 32768, 'r': 0.04   },\n",
    "        'acc_yaw':      {'bstar': 2047,     'rstar': 1,     'b': 2047,  'r': 0.1    },\n",
    "        'brk_trq_elec': {'bstar': 4096,     'rstar': -1,    'b': 4098,  'r': -1     },\n",
    "        'whl_trq_est':  {'bstar': 12800,    'rstar': 0.5,   'b': 12700, 'r': 1      },\n",
    "        'trac_cons':    {'bstar': 80,       'rstar': 1,     'b': 79,    'r': 1      },\n",
    "        'trip_cons':    {'bstar': 0,        'rstar': 0.1,   'b': 0,     'r': 1      }\n",
    "    }\n",
    "\n",
    "def convertdata(data, parameter):\n",
    "    bstar = parameter['bstar']\n",
    "    rstar = parameter['rstar']\n",
    "    b = parameter['b']\n",
    "    r = parameter['r']\n",
    "    # We only convert data in the second column at idx 1 (wrt. 0-indexing), as the first column is time\n",
    "    col0 = data[:,0]\n",
    "    col1 = ((data[:,1]-bstar*rstar)-b)*r\n",
    "    data = np.column_stack((col0, col1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def unpack_hdf5(hdf5_file, convert: bool = False):\n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        print(f)\n",
    "        data = unpack_hdf5_(f, convert)\n",
    "    return data\n",
    "\n",
    "\n",
    "def unpack_hdf5_(group, convert: bool = False):\n",
    "    data = {}\n",
    "    for key in group.keys():\n",
    "        if isinstance(group[key], h5py.Group):\n",
    "            data[key] = unpack_hdf5_(group[key])\n",
    "        else:\n",
    "            if convert and key in parameter_dict:\n",
    "                data[key] = convertdata(group[key][()], parameter_dict[key])\n",
    "            else:\n",
    "                d = group[key][()]\n",
    "                if isinstance(d, bytes):\n",
    "                    data[key] = d.decode('utf-8')\n",
    "                else:\n",
    "                    data[key] = group[key][()]\n",
    "    return data\n",
    "\n",
    "\n",
    "def find_best_start_and_end_indeces_by_lonlat(trip: np.ndarray, section: np.ndarray):\n",
    "    # Find the start and end indeces of the section data that are closest to the trip data\n",
    "    lon_a, lat_a = trip[:,0], trip[:,1]\n",
    "    lon_b, lat_b = section[:,0], section[:,1]\n",
    "    \n",
    "    start_index = np.argmin(np.linalg.norm(np.column_stack((lon_a, lat_a)) - np.array([lon_b[0], lat_b[0]]), axis=1))\n",
    "    end_index = np.argmin(np.linalg.norm(np.column_stack((lon_a, lat_a)) - np.array([lon_b[-1], lat_b[-1]]), axis=1))\n",
    "\n",
    "    return start_index, end_index\n",
    "\n",
    "\n",
    "def time_to_drive_X_meters(speed_kmh, distance_m=10):\n",
    "    # Convert speed from km/h to m/s\n",
    "    speed_ms = speed_kmh * (1000 / 3600)\n",
    "\n",
    "    # Calculate time in seconds\n",
    "    time_seconds = distance_m / speed_ms\n",
    "    \n",
    "    return time_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"platoon_CPH1_HH.hdf5\" (mode r)>\n"
     ]
    }
   ],
   "source": [
    "autopi_hh = unpack_hdf5('../data/raw/AutoPi_CAN/platoon_CPH1_HH.hdf5')\n",
    "gm_data = autopi_hh['GM']['16006']['pass_1'] # TODO choose a one true route. \n",
    "p79_hh = pd.read_csv('../data/raw/ref_data/cph1_zp_hh.csv', sep=';', encoding='unicode_escape')\n",
    "p79_vh = pd.read_csv('../data/raw/ref_data/cph1_zp_vh.csv', sep=';', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = h5py.File('../data/processed/segments.hdf5', 'r')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['0', '1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '19', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '2', '20', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '21', '210', '211', '212', '213', '214', '215', '216', '217', '218', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/10\" (55 members)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[\"10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '6', '7', '8', '9']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[\"10\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16006'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments.keys()\n",
    "segments[\"10\"].attrs[\"trip_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['aran', 'gm', 'p79']>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[\"10\"][\"10\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"gm\": shape (250, 42), type \"<f8\">"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[\"10\"][\"10\"][\"gm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55.71061985, 55.71061963, 55.71061942, 55.7106192 , 55.71061898,\n",
       "       55.71061875, 55.71061854, 55.71061832, 55.7106181 , 55.71061788,\n",
       "       55.71061766, 55.71061744, 55.71061723, 55.71061701, 55.71061679,\n",
       "       55.71061657, 55.71061635, 55.71061614, 55.71061592, 55.7106157 ,\n",
       "       55.71061548, 55.71061526, 55.71061505, 55.71061483, 55.71061461,\n",
       "       55.71061439, 55.71061418, 55.71061396, 55.71061374, 55.71061352,\n",
       "       55.71061331, 55.71061309, 55.71061287, 55.71061266, 55.71061244,\n",
       "       55.71061222, 55.710612  , 55.71061179, 55.71061157, 55.71061135,\n",
       "       55.71061114, 55.71061092, 55.7106107 , 55.71061048, 55.71061027,\n",
       "       55.71061005, 55.71060983, 55.71060962, 55.7106094 , 55.71060918,\n",
       "       55.71060897, 55.71060875, 55.71060853, 55.71060832, 55.7106081 ,\n",
       "       55.71060788, 55.71060767, 55.71060745, 55.71060723, 55.71060702,\n",
       "       55.7106068 , 55.71060659, 55.71060637, 55.71060615, 55.71060594,\n",
       "       55.71060572, 55.71060551, 55.71060529, 55.71060507, 55.71060486,\n",
       "       55.71060464, 55.71060443, 55.71060421, 55.710604  , 55.71060378,\n",
       "       55.71060356, 55.71060335, 55.71060313, 55.71060292, 55.7106027 ,\n",
       "       55.71060249, 55.71060227, 55.71060205, 55.71060184, 55.71060162,\n",
       "       55.71060141, 55.71060119, 55.71060098, 55.71060076, 55.71060055,\n",
       "       55.71060033, 55.71060011, 55.71059989, 55.71059967, 55.71059944,\n",
       "       55.71059921, 55.71059898, 55.71059875, 55.71059853, 55.7105983 ,\n",
       "       55.71059807, 55.71059784, 55.71059761, 55.71059739, 55.71059716,\n",
       "       55.71059693, 55.7105967 , 55.71059648, 55.71059625, 55.71059602,\n",
       "       55.71059579, 55.71059556, 55.71059534, 55.71059511, 55.71059488,\n",
       "       55.71059465, 55.71059442, 55.7105942 , 55.71059397, 55.71059374,\n",
       "       55.71059351, 55.71059329, 55.71059306, 55.71059283, 55.7105926 ,\n",
       "       55.71059237, 55.71059215, 55.71059192, 55.71059169, 55.71059146,\n",
       "       55.71059124, 55.71059101, 55.71059078, 55.71059056, 55.71059033,\n",
       "       55.7105901 , 55.71058987, 55.71058965, 55.71058942, 55.71058919,\n",
       "       55.71058897, 55.71058874, 55.71058851, 55.71058828, 55.71058806,\n",
       "       55.71058783, 55.7105876 , 55.71058738, 55.71058715, 55.71058692,\n",
       "       55.7105867 , 55.71058647, 55.71058624, 55.71058602, 55.71058579,\n",
       "       55.71058556, 55.71058533, 55.71058511, 55.71058488, 55.71058465,\n",
       "       55.71058443, 55.7105842 , 55.71058397, 55.71058375, 55.71058352,\n",
       "       55.7105833 , 55.71058307, 55.71058284, 55.71058262, 55.71058239,\n",
       "       55.71058217, 55.71058194, 55.71058172, 55.71058149, 55.71058127,\n",
       "       55.71058104, 55.71058081, 55.71058059, 55.71058036, 55.71058014,\n",
       "       55.71057991, 55.71057969, 55.71057946, 55.71057924, 55.71057901,\n",
       "       55.71057879, 55.71057856, 55.71057834, 55.71057811, 55.71057789,\n",
       "       55.71057766, 55.71057744, 55.71057721, 55.71057699, 55.71057676,\n",
       "       55.71057654, 55.71057631, 55.71057609, 55.71057586, 55.71057564,\n",
       "       55.71057541, 55.71057519, 55.71057496, 55.71057474, 55.71057451,\n",
       "       55.71057428, 55.71057406, 55.71057384, 55.71057361, 55.71057339,\n",
       "       55.71057316, 55.71057294, 55.71057271, 55.71057249, 55.71057227,\n",
       "       55.71057204, 55.71057182, 55.71057159, 55.71057137, 55.71057115,\n",
       "       55.71057092, 55.7105707 , 55.71057048, 55.71057025, 55.71057003,\n",
       "       55.7105698 , 55.71056958, 55.71056936, 55.71056913, 55.71056891,\n",
       "       55.71056869, 55.71056846, 55.71056824, 55.71056802, 55.71056779,\n",
       "       55.71056757, 55.71056735, 55.71056712, 55.7105669 , 55.71056668,\n",
       "       55.71056645, 55.71056623, 55.71056601, 55.71056578, 55.71056556,\n",
       "       55.71056534, 55.71056512, 55.71056489, 55.71056467, 55.71056445])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[\"10\"][\"10\"][\"gm\"][:,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments[\"10\"][\"10\"][\"gm\"][:,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments[\"10\"][\"10\"][\"gm\"][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226\n",
      "[[12.53019946, 55.7111808], [12.53018721, 55.71117492], [12.53017498, 55.71116905], [12.53016273, 55.71116317], [12.53015047, 55.7111573]]\n"
     ]
    }
   ],
   "source": [
    "# HH \n",
    "# Extract every 10th item starting from idx[0] to idx[1]+1, to get one location for each meter\n",
    "lon_zp_hh = p79_hh['Lon']\n",
    "lat_zp_hh = p79_hh['Lat']\n",
    "idx_hh = find_best_start_and_end_indeces_by_lonlat(p79_hh[['Lat', 'Lon']].to_numpy(), gm_data['gps'][:,1:])\n",
    "loc_hh_lon = lon_zp_hh[idx_hh[0]:idx_hh[1]+1:10]\n",
    "loc_hh_lat = lat_zp_hh[idx_hh[0]:idx_hh[1]+1:10]\n",
    "\n",
    "# Combine lon and lat into a list of lists\n",
    "loc_hh = [[lon, lat] for lon, lat in zip(loc_hh_lon, loc_hh_lat)]\n",
    "\n",
    "# For demonstration, let's print the first few pairs to verify\n",
    "print(len(loc_hh))\n",
    "print(loc_hh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226\n",
      "[[12.53019946, 55.7111808], [12.53018721, 55.71117492], [12.53017498, 55.71116905], [12.53016273, 55.71116317], [12.53015047, 55.7111573]]\n"
     ]
    }
   ],
   "source": [
    "# VH\n",
    "# Extract every 10th item starting from idx[0] to idx[1]+1, to get one location for each meter\n",
    "lon_zp_vh = p79_vh['Lon']\n",
    "lat_zp_vh = p79_vh['Lat']\n",
    "idx_vh = find_best_start_and_end_indeces_by_lonlat(p79_vh[['Lat', 'Lon']].to_numpy(), gm_data['gps'][:,1:])\n",
    "loc_vh_lon = lon_zp_vh[idx_hh[0]:idx_vh[1]+1:10]\n",
    "loc_vh_lat = lat_zp_vh[idx_hh[0]:idx_vh[1]+1:10]\n",
    "\n",
    "# Combine lon and lat into a list of lists\n",
    "loc_vh = [[lon, lat] for lon, lat in zip(loc_hh_lon, loc_hh_lat)]\n",
    "\n",
    "# For demonstration, let's print the first few pairs to verify\n",
    "print(len(loc_vh))\n",
    "print(loc_vh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16006': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [100, 0, 0]},\n",
       " '16008': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [100, 0, 0]},\n",
       " '16009': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [100, 0, 0]},\n",
       " '16010': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [100, 0, 0]},\n",
       " '16011': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [100, 0, 0]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_mapping_to_the_two_best_seconds_for_each_trip = {0: {\"16006\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]},\n",
    "                                                          \"16008\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]},\n",
    "                                                          \"16009\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]},\n",
    "                                                          \"16010\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]}, \n",
    "                                                          \"16011\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]}},\n",
    "                                                    1: {\"16006\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]},\n",
    "                                                          \"16008\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]},\n",
    "                                                          \"16009\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]},\n",
    "                                                          \"16010\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]}, \n",
    "                                                          \"16011\": {\"distance_segment_second_1\": [100, 0, 0], \"distance_segment_second_2\": [100, 0, 0]}}}\n",
    "ex_mapping_to_the_two_best_seconds_for_each_trip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16006': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [200, 0, 0]},\n",
       " '16008': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [200, 0, 0]},\n",
       " '16009': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [200, 0, 0]},\n",
       " '16010': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [200, 0, 0]},\n",
       " '16011': {'distance_segment_second_1': [100, 0, 0],\n",
       "  'distance_segment_second_2': [200, 0, 0]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intialize the mapping dictionaty\n",
    "all_trip_names = ['16006', '16008', '16009', '16010', '16011']\n",
    "mapping_to_the_two_best_seconds_for_each_trip = {}\n",
    "\n",
    "for index in range(len(loc_hh)):\n",
    "    trip_data = {} \n",
    "    for trip_name in all_trip_names:\n",
    "        # Each trip_name gets a dictionary with the two specified keys, each mapping to [100, 0, 0]\n",
    "        trip_data[trip_name] = {\n",
    "            \"distance_segment_second_1\": [100, 0, 0],\n",
    "            \"distance_segment_second_2\": [200, 0, 0]\n",
    "        }\n",
    "        mapping_to_the_two_best_seconds_for_each_trip[index] = trip_data\n",
    "mapping_to_the_two_best_seconds_for_each_trip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 10/1226 [01:03<2:08:49,  6.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now fill out the dictionary with the correct values\n",
    "for index, real_location in tqdm(enumerate(loc_hh), total=len(loc_hh), desc=\"Processing\"):\n",
    "    if index == 10:\n",
    "        break \n",
    "    for segment in segments.keys():\n",
    "        current_trip_name = segments[str(segment)].attrs[\"trip_name\"]\n",
    "        current_best_trip_1 = mapping_to_the_two_best_seconds_for_each_trip[index][current_trip_name]['distance_segment_second_1']\n",
    "        current_best_trip_2 = mapping_to_the_two_best_seconds_for_each_trip[index][current_trip_name]['distance_segment_second_2']\n",
    "        for second in segments[str(segment)].keys():\n",
    "            current_second = segments[str(segment)][str(second)]\n",
    "            current_second_lat = current_second[\"gm\"][:,15]\n",
    "            current_second_lon = current_second[\"gm\"][:,16]\n",
    "            current_second_locations = [[lon, lat] for lon, lat in zip(current_second_lon, current_second_lat)]\n",
    "            closest_sample_arg = np.argmin(np.linalg.norm(np.column_stack((current_second_lon, current_second_lat)) - np.array([real_location[0], real_location[1]]), axis=1))\n",
    "            best_at_second = current_second_locations[closest_sample_arg]\n",
    "            distance = np.linalg.norm(np.array(best_at_second) - np.array(real_location))\n",
    "            \n",
    "            if distance < current_best_trip_1[0]:\n",
    "                current_best_trip_2 = current_best_trip_1\n",
    "                current_best_trip_1 = [distance, segment, second]\n",
    "                mapping_to_the_two_best_seconds_for_each_trip[index][current_trip_name]['distance_segment_second_1'] = current_best_trip_1\n",
    "                mapping_to_the_two_best_seconds_for_each_trip[index][current_trip_name]['distance_segment_second_2'] = current_best_trip_2\n",
    "                \n",
    "            elif distance < current_best_trip_2[0]:\n",
    "                current_best_trip_2 = [distance, segment, second]\n",
    "                mapping_to_the_two_best_seconds_for_each_trip[index][current_trip_name]['distance_segment_second_2'] = current_best_trip_2\n",
    "            \n",
    "            else: \n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16006': {'distance_segment_second_1': [2.3317746094376914e-05, '0', '0'],\n",
       "  'distance_segment_second_2': [2.3883416921457747e-05, '7', '1']},\n",
       " '16008': {'distance_segment_second_1': [1.0869581973696451e-05, '28', '2'],\n",
       "  'distance_segment_second_2': [1.1523051548351325e-05, '33', '1']},\n",
       " '16009': {'distance_segment_second_1': [5.024776531263841e-06, '42', '2'],\n",
       "  'distance_segment_second_2': [1.8639465280960233e-05, '38', '2']},\n",
       " '16010': {'distance_segment_second_1': [4.628884016767617e-06, '64', '11'],\n",
       "  'distance_segment_second_2': [1.2269908010988185e-05, '64', '10']},\n",
       " '16011': {'distance_segment_second_1': [7.043385951903976e-07, '75', '11'],\n",
       "  'distance_segment_second_2': [1.8116235160918723e-05, '69', '14']}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_to_the_two_best_seconds_for_each_trip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# mapping_to_the_two_best_seconds_for_each_trip is our dictionary\n",
    "data = {\n",
    "    'Trip ID': [],\n",
    "    'Segment': [],\n",
    "    'Distance Segment Second 1': [],\n",
    "    'Distance Segment Second 2': []\n",
    "}\n",
    "\n",
    "for trip_id, segments in mapping_to_the_two_best_seconds_for_each_trip.items():\n",
    "    for segment_id, values in segments.items():\n",
    "        data['Trip ID'].append(trip_id)\n",
    "        data['Segment'].append(segment_id)\n",
    "        data['Distance Segment Second 1'].append(values['distance_segment_second_1'])\n",
    "        data['Distance Segment Second 2'].append(values['distance_segment_second_2'])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = 'trip_data_hh.csv'\n",
    "df.to_csv(csv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_filename = 'trip_data_hh.hdf5'\n",
    "\n",
    "# Create a new HDF5 file\n",
    "with h5py.File(hdf5_filename, 'w') as hdf:\n",
    "    for trip_id, segments in ex_mapping_to_the_two_best_seconds_for_each_trip.items():\n",
    "        group = hdf.create_group(str(trip_id))\n",
    "        for segment_id, segment_data in segments.items():\n",
    "            # Convert each segment's data into a numpy array for easier handling\n",
    "            ds_data = np.array([segment_data['distance_segment_second_1'],\n",
    "                                segment_data['distance_segment_second_2']])\n",
    "            # Each dataset is named after the segment ID\n",
    "            group.create_dataset(segment_id, data=ds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After mapping each AOI to 2 time windows for each car, we will predict the KPI of the AOI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 1226 AOIs. That is one every meter based on \n",
    "\n",
    "We have 10 windows for each AOI; 2 for each car and we have 5 cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the speed to estimate the amount of sample points there are in the area of interest (AOI)\n",
    "# The AOI is thought to be defined by an X meter long segment\n",
    "A_speed = 50\n",
    "B_speed = 30\n",
    "C_speed = 40\n",
    "\n",
    "sample_freq = 250  # Sample frequency in Hz\n",
    "A_samples = time_to_drive_X_meters(A_speed) * sample_freq\n",
    "B_samples = time_to_drive_X_meters(B_speed) * sample_freq\n",
    "C_samples = time_to_drive_X_meters(C_speed) * sample_freq\n",
    "total_samples = A_samples + B_samples + C_samples\n",
    "\n",
    "A_weight = A_samples / total_samples\n",
    "B_weight = B_samples / total_samples\n",
    "C_weight = C_samples / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now localise the point of interest in the time domains.\n",
    "# It does not work to just take the first and last lon. and lat. and not the median either. \n",
    "# We need to be able to find the location fast, right now we will just \n",
    "\n",
    "# Should they all have a dictionary?\n",
    "A_t1_data = [[1,2,3,4,5,6,7,8,9,10],\n",
    "        [1,2,3,4,5,6,7,8,9,10],\n",
    "        [1,2,3,4,5,6,7,8,9,10],\n",
    "        [1,2,3,4,5,6,7,8,9,10],\n",
    "        [1,2,3,4,5,6,7,8,9,10],\n",
    "        [1,2,3,4,5,6,7,8,9,10]]\n",
    "A_t1_pred = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
